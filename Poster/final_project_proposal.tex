\documentclass[10pt,twocolumn,letterpaper]{article}

%\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Object Detection within a Robotic Application with YOLOv2}

\author{Chia-Wen Tsai\\
{\tt\small amytsai17@hotmail.com}
\and
Felizitas Kunz\\
{\tt\small felizitas.kunz@online.de }
\and
Christoph Caprano\\
{\tt\small Caprano@in.tum.de}
\and
Oskar Haller\\
{\tt\small o.haller@outlook.com}
%\and
%Team Member 5\\
%{\tt\small fifth@i1.org}
}


\maketitle
%\thispagestyle{empty}

%
% Proposal I
%
\section*{Project Proposal}

\section{Introduction}
	   We want to teach a Robot learn how to play the child game memory/pairs. Therefore, it should be able to detect playing cards, their position on a table as well as the images that they contain. For object detection YOLOv2-Real-Time-Object detection is used for transfer learning with datasets containing images of our playing cards.
    \subsection{Related Works}
        Applying deep learning for object detection is a broad field of research. Similarly, various firms and researchers try to apply vision techniques in robotic applications.\\
        One example that falls into that field is the Amazon Picking Challenge(APC) (\href{https://arxiv.org/pdf/1609.09475.pdf}{https://arxiv.org/pdf/1609.09475.pdf}). Thereby, most of the teams apply multiple camera views to prevent arising issues such as clutter, self-occlusions, and bad reflections.\\
        To enable communication between embedded devices ROS has proven to be suitable in many applications (MLA	
        Quigley, Morgan, et al. "ROS: an open-source Robot Operating System." 2009, \href{http://www.willowgarage.com/papers/ros-open-source-robot-operating-system}{http://www.willowgarage.com/papers/ros-open-source-robot-operating-system}).\\
        For the object detection part, we plan to do transfer learning with YOLO/Darknet, which should result in real-time detection. (YOLO9000: Better, Faster, Stronger, (Redmon, Joseph and Farhadi, Ali)
\href{https://arxiv.org/abs/1612.08242}{https://arxiv.org/abs/1612.08242})  
\section{Dataset}
	For the object recognition, we plan to take images of our playing cards, and label them manually.
	For transfer learning we can use any dataset that provides sufficient amount of data.
	A promising dataset which is suited for object detection and is available publicly is the PASCAL VOC Dataset (\href{http://host.robots.ox.ac.uk/pascal/VOC/}{http://host.robots.ox.ac.uk/pascal/VOC/}).\\
	Facing the playing cards from different angles might be a challenge, because a change in the angle reshapes the object from a camera perspective. Furthermore, reflection issues can cause noise in the input camera stream.
\section{Methodology}
	To achieve our goal, "fast object detection within a robotic use case", we use a stream of images as input. The stream comes from  a webcam mounted on the robotic arm. A RaspberryPi hands the stream to a computer where the images are processed.\\
	To enable real-time detection of the different playing cards as well as getting position data relative to the camera, we aim at implementing transfer learning with the Darknet/YoloV2 Architecture on our Dataset.\\
    Resource wise we plan to train our network on the Google Cloud Platform. For processing the images we use a Laptop. The Raspberry Pi takes in the camera stream and sends it further to the Laptop. Last but not least, Motor commands are sent from the Laptop via an Arduino Uno to the Robot (compare Figure 1).  
		\begin{figure}[h!]
		\centering
		\includegraphics[width=0.7\linewidth]{../arduino-braccio-robotic-arm}
		\caption{Braccio Robotic Arm}
		\label{fig:arduino-braccio-robotic-arm}
		\end{figure}

\section{Outcome}
    First priority is that the robot can classify the images on the playing cards. With the goal to be able to correctly identify pairs.\\
    For a first stage the Robot shall be able to detect the cards at predefined positions by taking a single picture of each card.\\
    As a next step single cards shall be recognizable anywhere on the table, possibly in real-time.\\
    As optional outcome the robot should be able to pick up a flipped faced down card, turn it around and build stacks of pairs.
      
    

{\small
\bibliographystyle{ieee}
\bibliography{bib}
}

\end{document}